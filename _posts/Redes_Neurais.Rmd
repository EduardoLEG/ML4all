% Redes Neurais Artificiais
% `r format(Sys.time(), "%d de %B de %Y")`


```{r, include = FALSE}
source("_defs.R")
```
   
```{r, include = FALSE}

library(knitr)
opts_chunk$set(cache = TRUE,
               warning = FALSE,
               message = FALSE,
               fig.align = "center")

```

# Introdução #

As Redes Neurais Artificiais (RNA’s) foram inspiradas no sistema neural biológico. Neste capítulo vamos tratar do assunto de forma puramente estatística sem fazer qualquer analogia ao funcionamento cerebral biológico.



# Intuição do backpropagation #

A ideia do algoritmo de *backpropagation* é calcular o gradiente das expressões através da aplicação da **regra da cadeia**. Entender esse aspecto é o ponto crucial para compreender o funcionamento das RNA’s. 

1) **Derivadas parciais** 

Suponha que desejamos calcular as derivadas parciais de $f(x,y) = xy$:
$$
f(x,y) = x y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = y \hspace{0.5in} \frac{\partial f}{\partial y} = x 
$$
Considerando, por exemplo, que $x=4$ e $y=-3$, temos $f(x,y) = -12$. A derivada parcial com relação a $x$ é $\frac{\partial f}{\partial x} = -3$, e nos indica que o aumento de $x$ em uma unidade, acarreta um decréscimo de $-3$ em $f$. Trata-se de uma taxa de variação. Outros exemplos de derivadas parciais úteis ao nosso propósito são:

* $f(x,y) = x + y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = 1 \hspace{0.5in} \frac{\partial f}{\partial y} = 1$

* $f(x,y) = \max(x, y) \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = \mathbb{1}(x >= y) \hspace{0.5in} \frac{\partial f}{\partial y} = \mathbb{1}(y >= x)$

2) **Vetor gradiente**

O gradiente é um vetor de derivas parciais em todas as direções do espaço de $f$, i.e.
$$\nabla f = \left[\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\right]$$


3) **Regra da cadeia** 

Considere agora um caso mais complexo em que $f(x,y,z) = (x+y)z$, e desejamos encontrar $\frac{\partial f}{\partial x}$. Poderíamos fazer essa operação diretamente na função, mas recomenda-se utilizar uma abordagem particular, que ajudará no entendimento do algoritmo de *backpropagation*. Inicialmente, veja que $f$ pode ser decomposta em duas partes:
$$f=qz \ \ \ \ \ \ \ \textrm{em que} \ \ \ \ \ \ \ q=(x+y)$$
Dessa forma, podemos, por exemplo, estudar diretamente o impacto de $q=(x+y)$ em $f$. Além da taxa de variação de $f$ com relação a $x$, $\frac{\partial f}{\partial x}$, através da **regra da cadeia**:
$$\frac{\partial f}{\partial x} = \frac{\partial f}{\partial q} \frac{\partial q}{\partial x}.$$ 

4) **Backpropagation**

O processo se inicia no final da rede, e através das derivadas parciais conseguimos quantificar a influência de cada peso, $w$, no erro. De posse dessa informação, somos capazes de atualizar os pesos a cada iteração.

<center><img src="erros.jpg" alt="Drawing" style="width: 620px;"/></center>

<center><img src="nos.jpg" alt="Drawing" style="width: 620px;"/></center>

# Número de camadas #

A escolha do número de camadas depende do problema em estudo: 

1) **Camada única:** é capaz de posicionar o hiperplano no espaço das entradas;

2) **Duas camadas (uma oculta):** capaz de descrever uma regra de decisão em somente uma região convexa do espaço;

3) **Três camadas (duas ocultas):** a partir de três camadas somos capazes de generalizar regiões arbitrárias do espaço.
<center>
<figure>
<img src="camadas.jpg" alt="Drawing" style="width: 620px;"/>    <figcaption>LEGENDA Take me to [[2](#cnn)] </figcaption>
</figure>
</center>

# Funções de ativação #

Toda função de ativação recebe um escalar e executa uma operação matemática fixa. Tais transformações impõem que os valores permaneçam limitados, deixando o algoritmo mais estável e veloz. Veremos, em seguida, as funções mais populares na arquitetura das RNA's. 

1) A **Função Sigmoide**, $\sigma(x) = \frac{1}{(1+e^{-x})}$, é usada historicamente pela sua relação com a taxa de ativação do neurônio: totalmente inativo (quando 0) à frequência máxima (quando assume o valor 1). 
<center>
<figure>
<img src="sigmoid.jpeg" alt="Drawing" style="width: 320px;"/>
<figcaption>LEGENDA [[1](#cnn1)] </figcaption>
</figure>
</center>
Na prática, ela não é muito usada, pois sua ativação pode "matar" o neurônio quando em 0 (impedindo que qualquer sinal passe por aquele caminho), ou saturar quando em 1 (os neurônios saturados não aprenderão corretamente a tarefa). 

2) A **Função Tangente hiperbólica (tanh)**, $tanh(x) = \frac{2}{1+e^{-2x}}-1$, “achata” os valores no intervalo $[-1,1]$ e padece do mesmo problema de saturação mencionado anteriormente. Entretanto, agora a saída é centrada em zero.  
<center><img src="tanh.jpeg" alt="Drawing" style="width: 320px;"/></center>
Observe que o neurônio tanh é simplesmente o sigmoide escalado, $tanh(x) = 2\sigma(2x) - 1$. Na prática, neurônios desse tipo trazem resultados melhores à rede quando comparado com o sigmoide.

3) A função **Rectified Linear Unit (ReLU)**, $f(x) = max(0,x)$, se tornou popular nos últimos anos. Ela simplesmente atribui como zero um valor negativo e o mantém inalterado quando positivo.
<center><img src="relu.jpeg" alt="Drawing" style="width: 320px;"/></center>
Alguns estudos indicam que tais neurônios aceleram a convergência do gradiente descende estocástico quando comparado com neurônios sigmoides e tanh. Argumenta-se que esse sucesso se deve a sua não saturação. Além disso, seu custo computacional é baixíssimo (não envolve inversões, exponenciais etc.). Entretanto, recomenda-se cautela em sua utilização, pois as unidades ReLU podem ser frágeis durante o treinamento. Por exemplo, um gradiente elevado fluindo através do neurônio pode atualizar os pesos de tal maneira que o mesmo nunca seja ativado novamente. Ou seja, o neurônio pode "morrer" irreversivelmente. Algumas extensões prometem amenizar esse problema, por exemplo **Leaky ReLU**, $f(x) = \mathbb{1}_{(x\leq0)}(\alpha x) + \mathbb{1}_{(x>0)}(x)$, em que $\alpha$ é uma pequena constante. Nesse caso, em vez de atribuir zero à função, quando $x<0$, considera-se um pequeno coeficiente angular, p.ex., 0,01.

 **Em resumo, qual tipo de neurônio devo escolher?** Utilize ReLU monitorando a taxa de unidades “mortas” na rede. Alternativamente, opte pela Tanh, e evite a Sigmoide.


# Referência #

 
[1]<a name="cnn1"></a> Karpathy, Andrej. 2015. **Neural Networks Part 1: Setting Up the Architecture**. Notes for CS231n Convolutional Neural Networks for Visual Recognition, Stanford University.

[2]<a name="cnn"></a> Gales, Mark. 2001. **Handouts 4 & 5: Multi-Layer Perceptron: Introduction and Training**, University of Cambridge.


